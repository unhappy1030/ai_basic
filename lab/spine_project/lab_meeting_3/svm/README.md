# 학습 계획

## 공부 순서와 학습 내용

### 1. 분류(Classification)와 회귀(Regression)

- 지도학습의 두 가지 주요 유형 이해

### 분류(Classification)

분류는 데이터를 미리 정의된 클래스나 카테고리로 구분하는 문제이다.

#### 특징

- 출력이 이산적(discrete)값
- 예시 :
  - 이메일 스팸 분류 (스팸/정상)
  - 이미지 분류 (고양이/강아지)
  - 질병 진단(양성/음성)

#### 주요 알고리즘

- 로지스틱 회귀
- 결정 트리
- SVM
- 신경망

### 회귀(Regression)

회귀는 연속적인 값을 예측하는 문제이다.

#### 특징

- 출력이 연속적(continuous)
- 예시:
  - 집 가격 예측
  - 날씨 온도 예측
  - 주식 가격 예측

#### 주요 알고리즘

- 선형 회귀
- 다항 회귀
- 릿지 회귀
- 라쏘 회귀

### 차이점 비교

#### 1) 출력값의 특성

- 분류 : 이산적 값(예: 0 또는 1, 클래스 레이블)
- 회귀 : 연속적 값(예: 실수값)

#### 2) 평가 지표

- 분류: 정확도(Accuracy), 정밀도(Precision), 재현율(Recall), F1-score
- 회귀: MSE(Mean Squared Error), RMSE, MAE(Mean Absolute Error), $R^2$

#### 3) 모델의 목적

- 분류: 데이터를 미리 정의된 클래스로 구분
- 회귀: 연속적인 값의 패턴을 찾아 예측

### 2. 베이즈 정리(Bayesian Theorem)

베이즈 정리는 "새로운 정보를 얻었을 때 우리의 믿음을 업데이트하는 방법"입니다.

#### 쉽게 이해하기

예를 들어보겠습니다:

- 당신이 감기에 걸렸다고 가정해보겠습니다 (사전 확률)
- 의사가 검사 결과를 알려줍니다 (새로운 정보)
- 이 정보를 바탕으로 실제 감기일 확률을 다시 계산합니다 (사후 확률)

#### 수식 설명

베이즈 정리는 다음과 같이 표현됩니다:

P(A|B) = P(B|A) × P(A) / P(B)

여기서:

- P(A|B): "B라는 정보를 알았을 때 A일 확률" (사후 확률)
  - 예: 검사 결과가 양성일 때 실제로 감기일 확률
- P(B|A): "A일 때 B라는 정보를 얻을 확률"
  - 예: 감기일 때 검사 결과가 양성으로 나올 확률
- P(A): "A일 확률"
  - 예: 감기 걸릴 확률
- P(B): "B라는 정보를 얻을 확률"
  - 예: 검사 결과가 양성일 확률

#### 실제 예시

1. **스팸 메일 필터링**

   - 사전 확률: 일반적인 이메일 중 스팸 비율
   - 새로운 정보: 이메일에 특정 단어가 포함됨
   - 업데이트: 해당 단어가 포함된 이메일이 스팸일 확률

2. **의료 진단**

   - 사전 확률: 일반적인 질병 발생률
   - 새로운 정보: 환자의 증상
   - 업데이트: 해당 증상이 있을 때 특정 질병일 확률

3. **날씨 예측**
   - 사전 확률: 과거 데이터 기반의 비 올 확률
   - 새로운 정보: 현재 기압, 습도 등
   - 업데이트: 현재 기상 조건에서 비 올 확률

#### 장단점

- 장점:

  - 새로운 정보를 얻을 때마다 예측을 개선할 수 있음
  - 불확실성을 명확하게 표현할 수 있음
  - 과거 경험을 활용할 수 있음

- 단점:
  - 처음 시작할 때 사전 확률 설정이 어려움
  - 계산이 복잡할 수 있음
  - 데이터가 적을 때는 과거 경험에 많이 의존

### 3. 생성 모델(Generative Model)과 가우시안 모델

#### 생성 모델이란?

생성 모델은 데이터의 분포를 학습하여 새로운 데이터를 생성할 수 있는 모델입니다.

##### 주요 특징

1. **데이터 생성 능력**

   - 학습된 데이터의 패턴을 이해하여 새로운 데이터 생성 가능
   - 예: 사람 얼굴 이미지 생성, 텍스트 생성, 음성 합성

2. **확률적 접근**
   - 데이터의 확률 분포를 모델링
   - 불확실성을 명시적으로 처리

#### 가우시안 모델 (정규분포)

가우시안 모델은 가장 기본적이고 널리 사용되는 확률 분포입니다.

##### 정규분포의 특징

1. **수학적 표현**

   - 확률밀도함수: $f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$
   - 평균(μ)과 표준편차(σ)로 완전히 결정됨

2. **주요 성질**
   - 대칭성: 평균을 중심으로 좌우 대칭
   - 68-95-99.7 규칙:
     - 평균 ± 1σ: 전체 데이터의 68%
     - 평균 ± 2σ: 전체 데이터의 95%
     - 평균 ± 3σ: 전체 데이터의 99.7%

#### 생성 모델의 종류

1. **가우시안 혼합 모델(GMM)**

   - 여러 개의 정규분포를 조합
   - 복잡한 데이터 분포 표현 가능
   - 클러스터링에 활용

2. **오토인코더**

   - 데이터의 압축된 표현을 학습
   - 새로운 데이터 생성 가능
   - 차원 축소에 활용

3. **GAN(Generative Adversarial Network)**
   - 생성자와 판별자의 경쟁을 통한 학습
   - 매우 현실적인 데이터 생성 가능
   - 이미지, 텍스트 등 다양한 분야에 활용

#### 실제 응용 사례

1. **이미지 생성**

   - 사람 얼굴 이미지 생성
   - 스타일 변환
   - 이미지 복원

2. **텍스트 생성**

   - 자연어 생성
   - 대화 시스템
   - 문서 요약

3. **음성 합성**
   - 음성 생성
   - 음성 변환
   - 음성 복원

#### 장단점

- 장점:

  - 새로운 데이터 생성 가능
  - 데이터의 확률적 특성 이해 가능
  - 다양한 응용 분야

- 단점:
  - 학습이 복잡하고 어려움
  - 많은 데이터 필요
  - 계산 비용이 높음

### 4. 판별 함수와 선형 분류기

- 선형 분류의 기본 개념
- 결정 경계의 이해
- 실습: 2차원 데이터로 선형 분류기 구현

### 5. K-최근접 이웃 알고리즘

- KNN의 작동 원리
- 거리 측정 방법
- 실습: scikit-learn으로 KNN 구현

### 6. 파라미터 추정 방법

- 최대 가능도 추정(MLE)
- 최대 사후 확률 추정(MAP)
- 실습: 간단한 파라미터 추정 예제

### 7. 로지스틱 회귀

- 로지스틱 함수의 이해
- 이진 분류에서의 로지스틱 회귀
- 실습: scikit-learn으로 로지스틱 회귀 구현

### 8. 서포트 벡터 머신

- SVM의 기본 개념
- 커널 트릭의 이해
- 실습: scikit-learn으로 SVM 구현

## 학습 방법

1. **이론 학습**

   - 교재나 온라인 강의로 개념 이해
   - 수학적 기초가 필요한 부분은 수식 유도 과정도 함께 학습

2. **코드 실습**

   - 각 주제별로 간단한 예제 코드 작성
   - scikit-learn 라이브러리 활용
   - 결과 시각화를 통한 이해

3. **실제 데이터 적용**

   - 각 알고리즘을 실제 데이터셋에 적용
   - 성능 평가 및 비교

4. **정리 및 복습**
   - 각 주제별로 핵심 개념 정리
   - 다른 알고리즘들과의 비교 분석

# Support Vector Machine : SVM

## SVM 상세 설명

### 기본 개념

SVM(Support Vector Machine)은 두 클래스를 구분하는 결정 경계(선)을 찾는 알고리즘입니다. 이때 각 클래스와 결정 경계 사이의 거리(마진)를 최대화하는 것이 목표입니다.

### 수학적 표현

1. 결정 경계의 표현

   - 일반식: ax + by = c
   - 벡터식: w^T d = c
     - w = [a, b]: 가중치 벡터
     - d = [x, y]: 데이터 포인트 벡터
   - 기하학적 표현: ||w|| ||d|| cos θ = c

2. 마진 계산
   - 상단 경계: w^T d = c + 1
   - 하단 경계: w^T d = c - 1
   - 점과 결정 경계 사이의 거리: |w^T d - c| / ||w||
3. 전체 마진
   - 상단까지의 거리: (c + 1 - c) / ||w|| = 1/||w||
   - 하단까지의 거리: (c - (c - 1)) / ||w|| = 1/||w||
   - 전체 마진 = 2/||w||

### 최적화 목표

마진을 최대화하기 위해 ||w||를 최소화하는 것이 SVM의 주요 목표입니다.

### 마진 최적화 문제 변환

1. 마진 변환 과정:
   - 원래 목표: 마진 2/||w|| 최대화
   - 2/||w|| 최대화
     = 1/||w|| 최대화 (2는 상수이므로 생략 가능)
     = 1/\(\sqrt{w^T w}\) 최대화
     = \(\sqrt{w^T w}\) 최소화 (역수 관계)
     = w^T w 최소화 (제곱근은 단조 증가 함수)
     = 1/2(w^T w) 최소화 (1/2는 미분 계산을 편하게 하기 위해 추가)

### 제약 조건의 통합

1. 원래 두 개의 조건:

   - 클래스 1일 때: w^T d_i ≥ c + 1
   - 클래스 -1일 때: w^T d_i ≤ c - 1

2. 하나의 부등식으로 통합:

   - s_i(c - w^T d_i) ≤ 0
   - 여기서 s_i는 클래스 레이블 (+1 또는 -1)

3. 제약 조건 정의:
   - 부등식의 좌변 s_i(c - w^T d_i)을 g_i로 정의
   - g_i ≤ 0 형태의 부등식(inequality constraint)

### 최적화 문제 해결 방법

SVM의 최적화 문제는 다음과 같이 정리됩니다:

1. 최적화 목표:
   - 마진 2/||w|| 최대화
   - 1/2(w^T w) 최소화로 변환

2. 제약 조건:
   - 클래스 1: w^T d_i ≥ c + 1
   - 클래스 -1: w^T d_i ≤ c - 1
   - 통합된 형태: s_i(c - w^T d_i) ≤ 0

이러한 제약 조건이 있는 최적화 문제를 해결하기 위해 Primal-Dual Interior Point Method (IPM)가 사용됩니다. 이 방법은 원래 문제(Primal)와 쌍대 문제(Dual)를 동시에 고려하여 최적해를 찾습니다.

### 최적화 방법론 관련 질문

1. SVM에서 최적의 결정 경계를 찾기 위해 마진을 최대화하는 과정이 최적화 문제로 표현된다고 하셨는데, 이 과정에서 Primal 문제와 Dual 문제가 각각 어떤 형태를 가지나요?

2. Primal-Dual Interior Point Method가 무엇인지 기초적인 개념부터 설명해주실 수 있을까요? 특히 'Primal'과 'Dual'이 각각 무엇을 의미하는지 궁금합니다.

3. SVM의 최적화 문제를 풀 때 Primal-Dual IPM은 실제로 어떻게 적용되나요? 예를 들어, 마진 최대화 과정에서 이 방법이 어떻게 사용되는지 궁금합니다.

## 랩미팅 질문

### 결정 경계(Decision Boundary) 관련 질문

1. ax + by = c라는 결정 경계를 어떻게 초기에 설정하나요? 이 파라미터들의 초기값은 어떤 기준으로 정해지나요?
- 전체 데이터 평균을 지나도록 c를 설정

2. 결정 경계를 찾는 과정에서 a, b, c 값들이 어떻게 점진적으로 업데이트되는지 설명해주실 수 있을까요?
- 경사 하강법 사용으로 점진적으로 a, b, c 업데이트

### 최적화 방법론 관련 질문

3. Primal 문제에서 Dual 문제로 전환하는 과정에서 라그랑주 승수(Lagrange multiplier)가 어떤 역할을 하는지 설명해주실 수 있을까요?

4. Interior Point Method가 기존의 최적화 방법들과 비교했을 때 어떤 장점이 있나요? 특히 SVM에서는 어떤 이점이 있는건가요?

5. Primal-Dual Interior Point Method에서 중심 경로(central path)를 따라가는 과정을 좀 더 직관적으로 설명해주실 수 있을까요?

### 실제 구현 관련 질문

6. 실제 구현 시에 Interior Point Method의 종료 조건(stopping criteria)은 어떻게 설정하는 것이 좋은가요?

7. 비선형 SVM에서 커널 트릭을 사용할 때, Primal-Dual Interior Point Method는 어떻게 수정되어야 하나요?

#### 질문 순서 제안
1. 먼저 결정 경계의 초기 설정에 대해 질문 (1, 2번)
2. 그 다음 최적화 방법론의 전체적인 흐름 이해 (3, 4번)
3. 마지막으로 구체적인 구현 방법에 대해 질문 (6, 7번)



# 질문 정리

Support Vector Machine

1. 결정 경계의 표현

   - 일반식: ax + by = c
   - 벡터식: w^T d = c
     - w = [a, b]: 가중치 벡터
     - d = [x, y]: 데이터 포인트 벡터
   - 기하학적 표현: ||w|| ||d|| cos θ = c

### 결정 경계(Decision Boundary) 관련 질문

Q1. ax + by = c라는 결정 경계를 어떻게 초기에 설정하나요? 이 파라미터들의 초기값은 어떤 기준으로 정해지나요?

2. 마진 계산
   - 상단 경계: w^T d = c + 1
   - 하단 경계: w^T d = c - 1
   - 점과 결정 경계 사이의 거리: |w^T d - c| / ||w||
3. 전체 마진
   - 상단까지의 거리: (c + 1 - c) / ||w|| = 1/||w||
   - 하단까지의 거리: (c - (c - 1)) / ||w|| = 1/||w||
   - 전체 마진 = 2/||w||

SVM의 최적화 문제

1. 최적화 목표:
   - 마진 2/||w|| 최대화
   - 1/2(w^T w) 최소화로 변환

2. 제약 조건:
   - 클래스 1: w^T d_i ≥ c + 1
   - 클래스 -1: w^T d_i ≤ c - 1
   - 통합된 형태: s_i(c - w^T d_i) ≤ 0

### 최적화 방법론 관련 질문

Q2. Primal-Dual Interior Point Method가 무엇인지 궁금합니다.